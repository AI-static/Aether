from typing import List, Dict, Any, Optional
from config.settings import global_settings
from datetime import datetime
import asyncio
import json

# Agno imports
from agno.agent import Agent
from agno.models.dashscope import DashScope
from agno.db.postgres import AsyncPostgresDb
from playwright.async_api import async_playwright

# å¯¼å…¥å¤–éƒ¨ Service
from services.connector_service import ConnectorService
from utils.logger import logger

# 1. æ•°æ®åº“è¿æ¥
db = AsyncPostgresDb(
    db_url=f"postgresql+asyncpg://{global_settings.database.user}:{global_settings.database.password}@{global_settings.database.host}:{global_settings.database.port}/{global_settings.database.name}")

# 2. æ¨¡å‹é…ç½®
resoning_model = DashScope(
    base_url=global_settings.external_service.aliyun_base_url,
    api_key=global_settings.external_service.aliyun_api_key,
    id="qwen-max-latest",
)

chat_model = DashScope(
    base_url=global_settings.external_service.aliyun_base_url,
    api_key=global_settings.external_service.aliyun_api_key,
    id="qwen-plus",
)

class XiaohongshuDeepAgent:
    """å°çº¢ä¹¦æ·±åº¦çˆ†æ¬¾åˆ†æä¸“å®¶"""

    def __init__(
            self,
            source_id: str = "system_user",
            playwright: Any = None,
            keywords: str = None
    ):
        self.connector_service = ConnectorService(source="system", source_id=source_id, playwright=playwright)
        self.keywords = keywords
        self.current_date = datetime.now().strftime("%Y-%m-%d")

        # === æ ¸å¿ƒå˜åŒ– 1ï¼šAgent ä¸å†æŒ‚è½½ tools ===
        # å®ƒç°åœ¨åªæ˜¯ä¸€ä¸ªçº¯ç²¹çš„åˆ†æå¤§è„‘
        self.agent = Agent(
            name="å°çº¢ä¹¦çˆ†æ¬¾æ¢é’ˆ",
            model=chat_model,
            instructions=[
                f"å½“å‰æ—¥æœŸ: {self.current_date}ã€‚",
                "ä½ æ˜¯ä¸€ä¸ªæ“…é•¿æŒ–æ˜çˆ†æ¬¾é€»è¾‘çš„ä¸“å®¶ã€‚",
                "ç”¨æˆ·å·²ç»ä¸ºä½ å‡†å¤‡å¥½äº†ã€æœç´¢ç»“æœã€‘å’Œã€ç¬”è®°è¯¦æƒ…ã€‘çš„æ•°æ®ã€‚",
                "è¯·ä½ ç›´æ¥é˜…è¯»è¿™äº›æ•°æ®ï¼Œå®Œæˆä»¥ä¸‹åˆ†æï¼š",
                "1. **æ·±åº¦è§£ç **ï¼šåˆ†æç¬”è®°æ ‡é¢˜å¦‚ä½•åˆ¶é€ ç„¦è™‘/æœŸå¾…ï¼Ÿé¦–å›¾æœ‰ä½•è§†è§‰å¸ç›ç‚¹ï¼Ÿè¯„è®ºåŒºç—›ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
                "2. **è¾“å‡ºçˆ†æ¬¾çš„è¯¦ç»†ä¿¡æ¯**ï¼šåŸºäºæ•°æ®ï¼Œç»™å‡ºåŸæ–‡æ•°æ®ä¸çˆ†æ¬¾åˆ†æã€‚"
                "3. **è¾“å‡ºè¡ŒåŠ¨æŒ‡å—**ï¼šåŸºäºæ•°æ®ï¼Œç”Ÿæˆ 3 ä¸ªå…·ä½“çš„çˆ†æ¬¾é€‰é¢˜æ–¹æ¡ˆå’Œå»ºè®®ã€‚"
            ],
            db=db,
            markdown=True,
            add_history_to_context=True,
            user_id=source_id,
        )

        # ç”¨äºç”Ÿæˆå…³é”®è¯çš„å°å· Agent (è½»é‡çº§)
        self.planner = Agent(model=resoning_model, description="å…³é”®è¯è£‚å˜åŠ©æ‰‹")

    # === æ ¸å¿ƒå˜åŒ– 2ï¼šå·¥å…·å˜æˆäº†æ™®é€šçš„ Python å¼‚æ­¥æ–¹æ³• ===
    # è¿™äº›æ–¹æ³•ä¸å†è¢« Agent è‡ªåŠ¨è°ƒç”¨ï¼Œè€Œæ˜¯è¢« Python é€»è¾‘æ˜¾å¼è°ƒç”¨

    async def _generate_keywords(self) -> List[str]:
        """å‰ç½®å·¥ä½œ Step 1: è£‚å˜å…³é”®è¯"""
        logger.info("æ­£åœ¨è£‚å˜å…³é”®è¯...")
        prompt = f"è¯·åŸºäºæ ¸å¿ƒè¯ã€Œ{self.keywords}ã€ï¼Œè£‚å˜å‡º 3 ä¸ªä¸åŒç»´åº¦çš„æœç´¢è¯ï¼ˆæ ¸å¿ƒè¯ã€åœºæ™¯è¯ã€ç—›ç‚¹è¯ï¼‰ã€‚åªè¿”å›é€—å·åˆ†éš”çš„å…³é”®è¯å­—ç¬¦ä¸²ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"
        resp = await self.planner.arun(prompt)
        # ç®€å•çš„æ¸…æ´—é€»è¾‘
        keywords = [k.strip() for k in resp.content.replace("ï¼Œ", ",").split(",") if k.strip()]
        return keywords[:3]  # ç¡®ä¿åªå–å‰3ä¸ª

    async def _run_search(self, keywords: List[str], limit: int = 10) -> List[Dict]:
        """å‰ç½®å·¥ä½œ Step 2: æ‰§è¡Œæœç´¢"""
        from models.connectors import PlatformType
        logger.info(f"æ­£åœ¨æ‰§è¡Œæœç´¢: {keywords}")

        raw_results = await self.connector_service.search_and_extract(
            platform=PlatformType.XIAOHONGSHU,
            keywords=keywords,
            limit=limit
        )

        all_notes = []
        for res in raw_results:
            if res.get("success"):
                all_notes.extend(res.get("data", []))

        # æŒ‰ç‚¹èµæ•°å€’åºï¼Œå–å‰ 10 ä¸ªæœ€æœ‰ä»·å€¼çš„
        sorted_notes = sorted(all_notes, key=lambda x: x.get("liked_count", 0), reverse=True)
        return sorted_notes[:10]

    async def _fetch_details(self, notes: List[Dict]) -> str:
        """å‰ç½®å·¥ä½œ Step 3: æŠ“å–è¯¦æƒ…å¹¶æ‹¼æ¥æˆæ–‡æœ¬"""
        from models.connectors import PlatformType

        # æå– URL
        urls = [n.get("full_url") for n in notes if n.get("full_url")]
        logger.info(f"æ­£åœ¨æŠ“å–è¯¦æƒ…ï¼Œå…± {len(urls)} ç¯‡")

        # get_note_details è¿”å›: [{url, success, data, method}, ...]
        details_results = await self.connector_service.get_note_details(
            urls=urls,
            platform=PlatformType.XIAOHONGSHU
        )

        # æ„å»º url -> detail æ˜ å°„
        details_map = {}
        for result in details_results:
            if result.get("success") and result.get("data"):
                details_map[result.get("url")] = result.get("data", {})

        # æ‹¼æ¥ context æ–‡æœ¬
        context_parts = []
        for i, note in enumerate(notes):
            url = note.get("full_url")
            detail = details_map.get(url, {})
            
            # æå–è¯¦æƒ…æ•°æ®ï¼ˆä½¿ç”¨æ–°çš„æ‰å¹³åŒ–å­—æ®µï¼‰
            title = detail.get("title") or note.get("title", "æœªçŸ¥æ ‡é¢˜")
            desc = detail.get("desc", "")
            
            # äº’åŠ¨æ•°æ®å·²ç»æ˜¯æ‰å¹³åŒ–çš„æ•´æ•°
            liked_count = detail.get("liked_count", note.get("liked_count", 0))
            collected_count = detail.get("collected_count", 0)
            comment_count = detail.get("comment_count", 0)
            
            # å›¾ç‰‡å’Œè¯„è®º
            images = detail.get("images", [])
            cover_url = images[0].get("url") if images else None
            comments = detail.get("comments", [])

            # æ ¼å¼åŒ–è¯„è®ºï¼ˆå‰3æ¡ï¼‰
            comment_str = ""
            if comments:
                top_comments = comments[:3]
                comment_texts = [
                    f"- {c.get('content', '')[:50]}..."
                    for c in top_comments if c.get("content")
                ]
                comment_str = "\n".join(comment_texts)
            else:
                comment_str = "æš‚æ— è¯„è®º"

            note_str = (
                f"ã€ç¬”è®° {i + 1}ã€‘\n"
                f"æ ‡é¢˜: {title}\n"
                f"å°é¢: {cover_url}\n"
                f"é“¾æ¥: {url}\n"
                f"äº’åŠ¨æ•°æ®: ç‚¹èµ{liked_count} | æ”¶è—{collected_count} | è¯„è®º{comment_count}\n"
                f"æ­£æ–‡å†…å®¹:\n{desc}\n\n"
                f"ç²¾é€‰è¯„è®º:\n{comment_str}\n"
                f"{'='*60}"
            )
            context_parts.append(note_str)

        return "\n\n".join(context_parts)

    async def analyze_trends_stream(self):
        """
        æµå¼ä»»åŠ¡å…¥å£ - ç¼–æ’é€»è¾‘
        """
        yield "ğŸš€ [Step 1] æ­£åœ¨è¿›è¡Œå…³é”®è¯è£‚å˜...\n"
        search_keywords = await self._generate_keywords()
        yield f" -> è£‚å˜ç»“æœ: {search_keywords}\n"

        yield "ğŸ” [Step 2] æ­£åœ¨å¤šçº¿ç¨‹å¹¶å‘æœç´¢...\n"
        top_notes = await self._run_search(search_keywords)
        yield f" -> ç­›é€‰å‡º {len(top_notes)} ç¯‡å¤´éƒ¨ç¬”è®°\n"

        if not top_notes:
            yield "âŒ æœªæœç´¢åˆ°æœ‰æ•ˆæ•°æ®ï¼Œä»»åŠ¡ç»ˆæ­¢ã€‚"
            return

        yield "ğŸ“– [Step 3] æ­£åœ¨é˜…è¯»ç¬”è®°è¯¦æƒ…...\n"
        context_data = await self._fetch_details(top_notes)

        yield "ğŸ§  [Step 4] æ•°æ®å‡†å¤‡å®Œæ¯•ï¼ŒAgent å¼€å§‹æ·±åº¦åˆ†æ...\n\n"

        # === æ ¸å¿ƒï¼šæŠŠå‡†å¤‡å¥½çš„æ•°æ®å–‚ç»™ Agent ===
        prompt = f"""
        ä»»åŠ¡æ ¸å¿ƒè¯ï¼š{self.keywords}

        ä»¥ä¸‹æ˜¯æˆ‘ä¸ºä½ é‡‡é›†åˆ°çš„æœ€æ–°æ•°æ®ï¼š
        {context_data}

        è¯·æ ¹æ® instructions å¼€å§‹åˆ†æã€‚
        """

        async for chunk in self.agent.arun(prompt, stream=True):
            if chunk and chunk.content:
                yield chunk.content


# --- ä¸»ç¨‹åº ---
async def main():
    start_time = datetime.now()
    print("=== å°çº¢ä¹¦å¤šç»´çˆ†æ¬¾åˆ†æä»»åŠ¡å¯åŠ¨ ===", flush=True)
    print(f"â° ä»»åŠ¡å¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}", flush=True)

    async with async_playwright() as p:
        try:
            analyzer = XiaohongshuDeepAgent(
                source_id="system",
                playwright=p,
                keywords="agenté¢è¯•"
            )

            print(f"[æ ¸å¿ƒè¯]: {analyzer.keywords}")
            print("-" * 80)

            async for content in analyzer.analyze_trends_stream():
                print(content, end="", flush=True)

            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()

            print("\n" + "-" * 80)
            print("[ä»»åŠ¡ç»“æŸ]")
            print(f"â° ä»»åŠ¡ç»“æŸæ—¶é—´: {end_time.strftime('%Y-%m-%d %H:%M:%S')}", flush=True)
            print(f"â±ï¸  ä»»åŠ¡è€—æ—¶: {duration:.2f} ç§’", flush=True)

        except Exception as e:
            print(f"\nè¿è¡Œæ—¶å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())