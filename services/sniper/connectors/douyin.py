# -*- coding: utf-8 -*-
"""æŠ–éŸ³è¿æ¥å™¨ - ä½¿ç”¨ session + browser + agent æ–¹å¼"""

import asyncio
import json
import time
from typing import Dict, Any, List, Optional
from pydantic import BaseModel, Field
from bs4 import BeautifulSoup

from .base import BaseConnector
from utils.logger import logger
from utils.oss import oss_client
from agentbay import ActOptions, ExtractOptions, CreateSessionParams, BrowserContext, BrowserOption, BrowserScreen, BrowserFingerprint
from models.connectors import PlatformType

class CheckLoginStatus(BaseModel):
    """æœç´¢ç»“æœæ¨¡å‹"""
    has_login: bool = Field(description="æ˜¯å¦å·²ç»ç™»é™†")

class SearchItems(BaseModel):
    """æœç´¢ç»“æœæ¨¡å‹"""
    title: str = Field(description="è§†é¢‘æ ‡é¢˜")
    url: str = Field(description="è§†é¢‘è·³è½¬é“¾æ¥")
    author: str = Field(description="ä½œè€…æ˜µç§°")
    liked_count: str = Field(description="ç‚¹èµæ•°")

class SearchResult(BaseModel):
    items: List[SearchItems] = Field(description="æ¡ç›®åˆ—è¡¨")

class CreatorsItems(BaseModel):
    """æœç´¢ç»“æœæ¨¡å‹"""
    user_id: str = Field(description="æŠ–éŸ³å·")
    author: str = Field(description="ä½œè€…æ˜µç§°")
    fans_count: int = Field(description="ç²‰ä¸æ•°,å•ä½(ä¸ª)")
    url: str = Field(description="ç”¨æˆ·ä¸»é¡µåœ°å€")

class CreatorsResult(BaseModel):
    """æœç´¢ç»“æœæ¨¡å‹"""
    items: List[CreatorsItems] = Field(description="æ¡ç›®åˆ—è¡¨")

class VideoDetail(BaseModel):
    """è§†é¢‘è¯¦æƒ…æ¨¡å‹"""
    title: str = Field(description="è§†é¢‘æ ‡é¢˜")
    description: str = Field(description="è§†é¢‘æè¿°")
    author: str = Field(description="ä½œè€…æ˜µç§°")
    liked_count: str = Field(description="ç‚¹èµæ•°")
    comment_count: str = Field(description="è¯„è®ºæ•°")
    share_count: str = Field(description="åˆ†äº«æ•°")
    repost_count: str = Field(description="è½¬å‘æ•°")


class DouyinConnector(BaseConnector):
    """æŠ–éŸ³è¿æ¥å™¨ - ä½¿ç”¨ AgentBay session + browser + agent"""

    # ç±»å˜é‡ï¼Œæ‰€æœ‰å®ä¾‹å…±äº«
    _login_tasks = {}

    def __init__(self, playwright):
        super().__init__(platform_name=PlatformType.DOUYIN, playwright=playwright)

    async def search_and_extract(
        self,
        keywords: List[str],
        limit: int = 20,
        user_id: Optional[str] = None,
        source: str = "default",
        source_id: str = "default",
        concurrency: int = 2
    ) -> List[Dict[str, Any]]:
        """æ‰¹é‡æœç´¢æŠ–éŸ³è§†é¢‘ï¼ˆæ”¹è¿›ç‰ˆï¼šå»æ‰ AI è§†è§‰ï¼Œç›´æ¥è§£æ HTMLï¼‰

        Args:
            keywords: æœç´¢å…³é”®è¯åˆ—è¡¨
            limit: æ¯ä¸ªå…³é”®è¯é™åˆ¶ç»“æœæ•°é‡
            user_id: å¯é€‰çš„ç”¨æˆ·IDè¿‡æ»¤
            source: æ¥æºæ ‡è¯†
            source_id: æ¥æºID
            concurrency: å¹¶å‘æ•°

        Returns:
            List[Dict]: æœç´¢ç»“æœåˆ—è¡¨
        """

        # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼ˆéœ€è¦ CDPï¼Œä½¿ç”¨ Playwright APIï¼‰
        async with self.with_session(source, source_id, connect_cdp=True) as (session, browser, context):

            async def _process(keyword: str, idx: int):
                """å¤„ç†å•ä¸ªå…³é”®è¯æœç´¢"""
                logger.info(f"[douyin] Searching keyword {idx + 1}/{len(keywords)}: {keyword}")

                try:
                    # 1. ä¿®æ­£ URLï¼šä½¿ç”¨æ­£ç¡®çš„æœç´¢æ ¼å¼
                    search_url = f"https://www.douyin.com/search/{keyword}?type=video"
                    logger.info(f"[douyin] Navigating to: {search_url}")

                    # 2. åˆ›å»ºæ–°é¡µé¢å¹¶å¯¼èˆª
                    page = await context.new_page()
                    try:
                        await page.goto(search_url, timeout=60000, wait_until="domcontentloaded")

                        # 3. å¢åŠ ç­‰å¾…æ—¶é—´ï¼Œè®©é¡µé¢å®Œå…¨åŠ è½½
                        await asyncio.sleep(5)

                        # 4. æ»šåŠ¨åŠ è½½æ›´å¤šå†…å®¹
                        scroll_count = 5
                        for i in range(scroll_count):
                            try:
                                await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                                await asyncio.sleep(2)
                                logger.info(f"[douyin] Scrolled {i + 1}/{scroll_count} times")
                            except Exception as e:
                                logger.warning(f"[douyin] Scroll failed at step {i + 1}: {e}")
                                break

                        # 5. è·å–é¡µé¢ HTML å†…å®¹
                        page_content = await page.content()
                        soup = BeautifulSoup(page_content, 'html.parser')
                        logger.info(f"[douyin] Page loaded, HTML length: {len(page_content)}")

                        # 6. ä½¿ç”¨ CSS é€‰æ‹©å™¨ç›´æ¥æå–è§†é¢‘æ•°æ®
                        videos_data = self._parse_search_videos(soup, limit, user_id)

                        if videos_data:
                            logger.info(f"[douyin] Found {len(videos_data)} videos for '{keyword}'")
                            return {
                                "keyword": keyword,
                                "success": True,
                                "data": videos_data
                            }
                        else:
                            logger.warning(f"[douyin] No videos found for '{keyword}'")
                            return {
                                "keyword": keyword,
                                "success": False,
                                "error": "No videos found in search results",
                                "data": []
                            }
                    finally:
                        await page.close()

                except Exception as e:
                    logger.error(f"[douyin] Error processing keyword '{keyword}': {e}")
                    import traceback
                    logger.error(f"[douyin] Traceback: {traceback.format_exc()}")
                    return {
                        "keyword": keyword,
                        "success": False,
                        "error": str(e),
                        "data": []
                    }

            # å¹¶å‘æ‰§è¡Œæœç´¢
            semaphore = asyncio.Semaphore(concurrency)

            async def worker(keyword, idx):
                async with semaphore:
                    return await _process(keyword, idx)

            tasks = [asyncio.create_task(worker(kw, idx)) for idx, kw in enumerate(keywords)]
            results = await asyncio.gather(*tasks)

            return results
        # é€€å‡ºä¸Šä¸‹æ–‡æ—¶è‡ªåŠ¨æ¸…ç† session å’Œ browser

    async def get_note_detail(
        self,
        urls: List[str],
        source: str = "default",
        source_id: str = "default",
        concurrency: int = 2
    ) -> List[Dict[str, Any]]:
        """æ‰¹é‡è·å–æŠ–éŸ³è§†é¢‘è¯¦æƒ…ï¼ˆä½¿ç”¨é€‰æ‹©å™¨å¿«é€Ÿæå–ï¼Œä¸ä½¿ç”¨ Agentï¼‰

        Args:
            urls: è§†é¢‘URLåˆ—è¡¨
            source: æ¥æºæ ‡è¯†
            source_id: æ¥æºID
            concurrency: å¹¶å‘æ•°

        Returns:
            List[Dict]: è§†é¢‘è¯¦æƒ…åˆ—è¡¨
        """

        # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼ˆéœ€è¦ CDPï¼Œä½¿ç”¨ Playwright APIï¼‰
        async with self.with_session(source, source_id, connect_cdp=True) as (session, browser, context):

            async def _process(url: str, idx: int):
                """å¤„ç†å•ä¸ªè§†é¢‘è¯¦æƒ…æå–"""
                logger.info(f"[douyin] Extracting detail {idx + 1}/{len(urls)}: {url}")

                try:
                    # 1. åˆ›å»ºæ–°é¡µé¢å¹¶å¯¼èˆª
                    page = await context.new_page()
                    try:
                        await page.goto(url, timeout=60000, wait_until="domcontentloaded")

                        # 2. ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½ï¼ˆç»™ JavaScript æ—¶é—´æ‰§è¡Œï¼‰
                        await asyncio.sleep(2)

                        await session.browser.agent.act(
                            ActOptions(
                            action="å¦‚æœé¡µé¢ä¸Šæœ‰å¼¹çª—ï¼Œè¯·å…³é—­å®ƒã€‚",
                            use_vision=True),
                            page)


                        # 3. é€šè¿‡ JavaScript ç›´æ¥è·å– window.__RENDER_DATA__ï¼ˆæœ€å¯é ï¼‰
                        js_data = None
                        try:
                            render_data_js = """
                            () => {
                                if (typeof window !== 'undefined' && window.__RENDER_DATA__) {
                                    return {type: 'render_data', data: window.__RENDER_DATA__};
                                        }
                                return {type: 'none'};
                            }
                            """
                            js_result = await page.evaluate(render_data_js)
                            if js_result and js_result.get('type') == 'render_data':
                                js_data = js_result
                                logger.info(f"[douyin] Found window.__RENDER_DATA__ via JS")
                        except Exception as e:
                            logger.debug(f"[douyin] JS extraction failed: {e}")

                        # 4. è·å–é¡µé¢ HTML å†…å®¹ä½œä¸ºå¤‡ä»½
                        page_content = await page.content()
                        soup = BeautifulSoup(page_content, 'html.parser')
                        logger.info(f"[douyin] Page loaded, HTML length: {len(page_content)}")

                        # 5. ä¼˜å…ˆä½¿ç”¨ JS æ•°æ®ï¼Œå¦åˆ™è§£æ HTML
                        video_data = self._parse_video_detail(soup, js_data)

                        if video_data:
                            logger.info(f"[douyin] Successfully extracted detail: {video_data.get('title', 'N/A')[:30]}")
                            return {
                                "url": url,
                                "success": True,
                                "data": video_data
                            }
                        else:
                            logger.warning(f"[douyin] Failed to extract detail for: {url}")
                            return {
                                "url": url,
                                "success": False,
                                "error": "Failed to parse video detail from HTML",
                                "data": {}
                            }
                    finally:
                        await page.close()

                except Exception as e:
                    logger.error(f"[douyin] Error processing URL '{url}': {e}")
                    import traceback
                    logger.error(f"[douyin] Traceback: {traceback.format_exc()}")
                    return {
                        "url": url,
                        "success": False,
                        "error": str(e),
                        "data": {}
                    }

            # å¹¶å‘æ‰§è¡Œæå–
            semaphore = asyncio.Semaphore(concurrency)

            async def worker(url, idx):
                async with semaphore:
                    return await _process(url, idx)

            tasks = [asyncio.create_task(worker(u, idx)) for idx, u in enumerate(urls)]
            results = await asyncio.gather(*tasks)

            return results
        # é€€å‡ºä¸Šä¸‹æ–‡æ—¶è‡ªåŠ¨æ¸…ç† session å’Œ browser

    async def harvest_user_content(
        self,
        creator_ids: List[str],
        limit: Optional[int] = None,
        source: str = "default",
        source_id: str = "default",
        concurrency: int = 2
    ) -> List[Dict[str, Any]]:
        """æ‰¹é‡æŠ“å–åˆ›ä½œè€…çš„è§†é¢‘å†…å®¹ï¼ˆæ”¹è¿›ç‰ˆï¼šå»æ‰ AI è§†è§‰ï¼Œå®¹æ˜“è¢«åçˆ¬ï¼Œç›´æ¥è§£æ HTMLï¼‰

        Args:
            creator_ids: åˆ›ä½œè€…IDæˆ–æ˜µç§°åˆ—è¡¨
            limit: æ¯ä¸ªåˆ›ä½œè€…é™åˆ¶è§†é¢‘æ•°é‡
            source: æ¥æºæ ‡è¯†
            source_id: æ¥æºID
            concurrency: å¹¶å‘æ•°

        Returns:
            List[Dict]: è§†é¢‘åˆ—è¡¨
        """

        # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼ˆéœ€è¦ CDPï¼Œä½¿ç”¨ Playwright APIï¼‰
        async with self.with_session(source, source_id, connect_cdp=True) as (session, browser, context):

            async def _process(creator_id: str, idx: int):
                """å¤„ç†å•ä¸ªåˆ›ä½œè€…çš„å†…å®¹æå–"""
                logger.info(f"[douyin] Harvesting content {idx + 1}/{len(creator_ids)}: {creator_id}")

                try:
                    # 1. ä¿®æ­£ URLï¼šä½¿ç”¨æ­£ç¡®çš„æœç´¢æ ¼å¼
                    search_url = f"https://www.douyin.com/search/{creator_id}?type=user"
                    logger.info(f"[douyin] Navigating to: {search_url}")

                    # 2. åˆ›å»ºæ–°é¡µé¢å¹¶å¯¼èˆª
                    page = await context.new_page()
                    try:
                        await page.goto(search_url, timeout=60000, wait_until="domcontentloaded")

                        # 3. å¢åŠ ç­‰å¾…æ—¶é—´ï¼Œè®©é¡µé¢å®Œå…¨åŠ è½½
                        await asyncio.sleep(5)

                        # 4. æ»šåŠ¨åŠ è½½æ›´å¤šå†…å®¹ï¼ˆæ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸ºï¼‰
                        scroll_count = 5
                        for i in range(scroll_count):
                            try:
                                # ä½¿ç”¨ JavaScript æ»šåŠ¨åˆ°åº•éƒ¨
                                await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                                # æ¯æ¬¡æ»šåŠ¨åç­‰å¾… 2 ç§’
                                await asyncio.sleep(2)
                                logger.info(f"[douyin] Scrolled {i + 1}/{scroll_count} times")
                            except Exception as e:
                                logger.warning(f"[douyin] Scroll failed at step {i + 1}: {e}")
                                break

                        # 5. è·å–é¡µé¢ HTML å†…å®¹
                        page_content = await page.content()
                        soup = BeautifulSoup(page_content, 'html.parser')
                        logger.info(f"[douyin] Page loaded, HTML length: {len(page_content)}")

                        # 6. ä½¿ç”¨ CSS é€‰æ‹©å™¨ç›´æ¥æå–ç”¨æˆ·æ•°æ®
                        users_data = self._parse_search_users(soup, limit)

                        if users_data:
                            logger.info(f"[douyin] Found {len(users_data)} users for '{creator_id}'")
                            return {
                                "creator_id": creator_id,
                                "success": True,
                                "data": users_data
                            }
                        else:
                            logger.warning(f"[douyin] No users found for '{creator_id}'")
                            return {
                                "creator_id": creator_id,
                                "success": False,
                                "error": "No users found in search results",
                                "data": []
                            }
                    finally:
                        await page.close()

                except Exception as e:
                    logger.error(f"[douyin] Error harvesting from '{creator_id}': {e}")
                    import traceback
                    logger.error(f"[douyin] Traceback: {traceback.format_exc()}")
                    return {
                        "creator_id": creator_id,
                        "success": False,
                        "error": str(e),
                        "data": []
                    }

            # å¹¶å‘æ‰§è¡Œé‡‡æ”¶
            semaphore = asyncio.Semaphore(concurrency)

            async def worker(creator_id, idx):
                async with semaphore:
                    return await _process(creator_id, idx)

            tasks = [asyncio.create_task(worker(cid, idx)) for idx, cid in enumerate(creator_ids)]
            results = await asyncio.gather(*tasks)

            return results
        # é€€å‡ºä¸Šä¸‹æ–‡æ—¶è‡ªåŠ¨æ¸…ç† session å’Œ browser

    async def login_with_cookies(
        self,
        cookies: Dict[str, str],
        source: str = "default",
        source_id: str = "default"
    ) -> str:
        """ä½¿ç”¨ Cookie ç™»å½•æŠ–éŸ³"""
        context_key = self._build_context_id(source, source_id)
        logger.info(f"[douyin] Logging in with context_id: {context_key}")

        # è·å–æˆ–åˆ›å»º Context
        context_res = await self.agent_bay.context.get(context_key, create=True)
        if not context_res.success:
            raise ValueError(f"Failed to create context: {context_res.error_message}")

        # åˆ›å»ºä¸´æ—¶ Session è¿›è¡Œ Cookie æ³¨å…¥
        session_res = await self.agent_bay.create(
            CreateSessionParams(
                image_id="browser_latest",
                browser_context=BrowserContext(context_res.context.id, auto_upload=True)
            )
        )
        if not session_res.success:
            raise ValueError(f"Failed to create session: {session_res.error_message}")

        session = session_res.session
        browser = None

        try:
            await session.browser.initialize(BrowserOption(
                screen=BrowserScreen(width=1920, height=1080),
                solve_captchas=True,
                use_stealth=True,
                fingerprint=BrowserFingerprint(
                    devices=["desktop"],
                    operating_systems=["windows"],
                    locales=self.get_locale(),
                ),
            ))
            browser, context_p = await self._connect_cdp(session)
            page = await context_p.new_page()

            # è½¬æ¢å¹¶æ³¨å…¥ Cookies
            cookies_list = [{
                "name": k, "value": v, "domain": ".douyin.com", "path": "/",
                "httpOnly": False, "secure": False, "expires": int(time.time()) + 86400
            } for k, v in cookies.items()]

            await context_p.add_cookies(cookies_list)
            await asyncio.sleep(0.5)

            # éªŒè¯ç™»å½•
            await page.goto("https://www.douyin.com", timeout=60000)
            await asyncio.sleep(1)

            # æ£€æŸ¥ç™»å½•çŠ¶æ€
            is_logged_in = await self._check_login_status_douyin(page)
            logger.info(f"[douyin] Login status: {is_logged_in}")

            if not is_logged_in:
                raise ValueError("Login failed: cookies invalid or expired")

            return context_res.context.id

        finally:
            await self.cleanup_resources(session, browser)

    async def _check_login_status_douyin(self, page) -> bool:
        """æ£€æŸ¥æŠ–éŸ³ç™»å½•çŠ¶æ€"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰ç”¨æˆ·å¤´åƒç­‰ç™»å½•æ ‡è¯†
            await page.wait_for_selector(".login-btn", timeout=3000)
            # å¦‚æœæ‰¾åˆ°ç™»å½•æŒ‰é’®ï¼Œè¯´æ˜æœªç™»å½•
            return False
        except:
            # å¦‚æœæ²¡æ‰¾åˆ°ç™»å½•æŒ‰é’®ï¼Œè¯´æ˜å·²ç™»å½•
            return True

    async def login_with_qrcode(
        self,
        source: str = "default",
        source_id: str = "default",
        timeout: int = 120
    ) -> Dict[str, Any]:
        """äºŒç»´ç ç™»å½•æŠ–éŸ³

        Args:
            source: æ¥æºæ ‡è¯†
            source_id: æ¥æºID
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

        Returns:
            Dict: åŒ…å«äºŒç»´ç å›¾ç‰‡ URL çš„å­—å…¸
        """
        context_key = self._build_context_id(source, source_id)
        logger.info(f"[douyin] QRCode login with context_id: {context_key}")

        # åˆ›å»ºæŒä¹…åŒ– context
        context_result = await self.agent_bay.context.get(context_key, create=True)
        if not context_result.success:
            raise ValueError(f"Failed to create context: {context_result.error_message}")

        # åˆ›å»º sessionï¼ˆauto_upload=Falseï¼Œæ‰‹åŠ¨æ§åˆ¶è½ç›˜æ—¶æœºï¼‰
        session_result = await self.agent_bay.create(
            CreateSessionParams(
                image_id="browser_latest",
                browser_context=BrowserContext(context_result.context.id, auto_upload=False)
            )
        )

        if not session_result.success:
            raise ValueError(f"Failed to create session: {session_result.error_message}")

        session = session_result.session

        try:
            # åˆå§‹åŒ–æµè§ˆå™¨
            ok = await session.browser.initialize(BrowserOption(
                screen=BrowserScreen(width=1920, height=1080),
                solve_captchas=True,
                use_stealth=True,
                fingerprint=BrowserFingerprint(
                    devices=["desktop"],
                    operating_systems=["windows"],
                    locales=self.get_locale(),
                ),
            ))

            if not ok:
                await self.agent_bay.delete(session, sync_context=False)
                raise RuntimeError("Failed to initialize browser")

            # å¯¼èˆªåˆ°æŠ–éŸ³ç™»å½•é¡µ
            await session.browser.agent.navigate("https://www.douyin.com")
            await asyncio.sleep(1.5)

            # æ£€æŸ¥æ˜¯å¦å·²ç™»å½•
            extract_options = ExtractOptions(
                instruction="""æŸ¥çœ‹æ­¤é¡µé¢ï¼Œåˆ¤æ–­ç”¨æˆ·æ˜¯å¦å·²ç»ç™»å½•æŠ–éŸ³ã€‚
å¦‚æœé¡µé¢é¡¶éƒ¨æœ‰ç”¨æˆ·å¤´åƒã€æ˜µç§°ç­‰ä¸ªäººä¿¡æ¯ï¼Œåˆ™ has_login ä¸º trueï¼Œå¦åˆ™ä¸º falseã€‚
é‡è¦ï¼šåªè¿”å› JSON æ ¼å¼ï¼Œä¸è¦è¿”å›å…¶ä»–æ–‡å­—è¯´æ˜ã€‚""",
                use_vision=True,
                schema=CheckLoginStatus
            )

            try:
                success, data = await session.browser.agent.extract(extract_options)
                if success and data.has_login:
                    return {
                        "success": True,
                        "context_id": context_key,
                        "message": "Has Logining",
                        "is_logged_in": True
                    }
            except Exception as e:
                logger.warning(f"[douyin] Failed to check login status: {e}, will continue to show QR code")
                # æ£€æŸ¥å¤±è´¥æ—¶ï¼Œç»§ç»­æ˜¾ç¤ºäºŒç»´ç ï¼ˆå‡è®¾æœªç™»å½•ï¼‰
                # ä¸ç«‹å³è¿”å›ï¼Œç»§ç»­æ‰§è¡Œåé¢çš„äºŒç»´ç æ˜¾ç¤ºé€»è¾‘

            # ä½¿ç”¨ Agent æ‰¾åˆ°å¹¶ç‚¹å‡»ç™»å½•æ–¹å¼ï¼Œæ˜¾ç¤ºäºŒç»´ç 
            login_act = ActOptions(
                action="""
                1. æŸ¥æ‰¾é¡µé¢ä¸Šçš„"ç™»å½•"æŒ‰é’®å¹¶ç‚¹å‡»
                2. åœ¨å¼¹å‡ºçš„ç™»å½•æ¡†ä¸­ï¼Œé€‰æ‹©"æ‰«ç ç™»å½•"æˆ–"äºŒç»´ç ç™»å½•"æ–¹å¼
                3. ç¡®ä¿äºŒç»´ç å·²æ˜¾ç¤ºåœ¨é¡µé¢ä¸Š
                """,
                use_vision=True
            )

            await session.browser.agent.act(login_act)
            await asyncio.sleep(2)

            # è·å–äºŒç»´ç å›¾ç‰‡ URL
            qrcode_url = session.resource_url

            if not qrcode_url:
                raise ValueError("Failed to get QR code URL")

            logger.info(f"[douyin] QRCode generated, waiting for scan...")

            # ğŸ”¥ å…³é”®ï¼šå¯åŠ¨åå°ä»»åŠ¡ï¼Œç›‘å¬ç™»å½•ç¡®è®¤ + è‡ªåŠ¨è½ç›˜
            asyncio.create_task(
                self._monitor_and_cleanup(
                    session=session,
                    context_id=context_key,
                    timeout=timeout
                )
            )

            return {
                "success": True,
                "context_id": context_key,
                "browser_url": qrcode_url,  # äº‘æµè§ˆå™¨ URL
                "qrcode": qrcode_url,  # å…¼å®¹æ—§å­—æ®µ
                "timeout": timeout,
                "message": "Cloud browser created, waiting for login",
                "is_logged_in": False
            }

        except Exception as e:
            logger.debug(f"[douyin] Check existing context failed: {e}")
            await self.cleanup_resources(session, None)
            await self.agent_bay.delete(session, sync_context=False)

    def _parse_search_users(self, soup: BeautifulSoup, limit: Optional[int] = None) -> List[Dict[str, Any]]:
        """ä»æœç´¢é¡µé¢è§£æç”¨æˆ·æ•°æ®ï¼ˆä½¿ç”¨ BeautifulSoupï¼‰

        Args:
            soup: BeautifulSoup è§£æåçš„é¡µé¢å¯¹è±¡
            limit: é™åˆ¶è¿”å›æ•°é‡

        Returns:
            List[Dict]: ç”¨æˆ·æ•°æ®åˆ—è¡¨
        """
        users_data = []

        try:
            # å‚è€ƒ undoom çš„é€‰æ‹©å™¨ï¼ŒæŸ¥æ‰¾ç”¨æˆ·å¡ç‰‡
            # å°è¯•å¤šä¸ªå¯èƒ½çš„é€‰æ‹©å™¨ï¼ˆæŠ–éŸ³é¡µé¢ç»“æ„å¯èƒ½å˜åŒ–ï¼‰
            possible_selectors = [
                "div.search-result-card > a.hY8lWHgA.poLTDMYS",  # undoom çš„é€‰æ‹©å™¨
                "a[data-e2e='search-user-card']",
                "div[class*='user'] a[href*='/user/']",
                "li[class*='user'] a[href*='/user/']",
            ]

            user_items = []
            for selector in possible_selectors:
                items = soup.select(selector)
                if items:
                    logger.info(f"[douyin] Found {len(items)} user items with selector: {selector}")
                    user_items = items
                    break

            if not user_items:
                logger.warning("[douyin] No user items found with any selector")
                # æ‰“å°éƒ¨åˆ† HTML ç”¨äºè°ƒè¯•
                logger.debug(f"[douyin] Page HTML preview: {str(soup)[:500]}")
                return []

            # è§£ææ¯ä¸ªç”¨æˆ·å¡ç‰‡
            for item in user_items[:limit] if limit else user_items:
                try:
                    user_data = self._extract_single_user(item)
                    if user_data and user_data.get('author'):  # è‡³å°‘è¦æœ‰ç”¨æˆ·å
                        users_data.append(user_data)
                except Exception as e:
                    logger.warning(f"[douyin] Failed to extract single user: {e}")
                    continue

            logger.info(f"[douyin] Successfully parsed {len(users_data)} users")

        except Exception as e:
            logger.error(f"[douyin] Error parsing search users: {e}")

        return users_data

    def _extract_single_user(self, item) -> Optional[Dict[str, Any]]:
        """ä»å•ä¸ªç”¨æˆ·å¡ç‰‡å…ƒç´ æå–æ•°æ®

        Args:
            item: BeautifulSoup å…ƒç´ å¯¹è±¡

        Returns:
            Dict: ç”¨æˆ·æ•°æ®ï¼Œå¤±è´¥è¿”å› None
        """
        try:
            # æå–ç”¨æˆ·å/æ˜µç§°ï¼ˆå°è¯•å¤šä¸ªé€‰æ‹©å™¨ï¼‰
            author = ""
            title_selectors = [
                "div.XQwChAbX p.v9LWb7QE span span span span span",
                "span[class*='title']",
                "div[class*='title']",
                "p[class*='name']",
            ]
            for selector in title_selectors:
                elem = item.select_one(selector)
                if elem:
                    author = elem.get_text(strip=True)
                    break

            # æå–ç”¨æˆ·é“¾æ¥
            url = ""
            link_elem = item.select_one('a') or item
            if link_elem and link_elem.get('href'):
                href = link_elem.get('href', '')
                if href.startswith('//'):
                    url = 'https:' + href
                elif href.startswith('/'):
                    url = 'https://www.douyin.com' + href
                else:
                    url = href

            # æå–æŠ–éŸ³å·ï¼ˆå°è¯•å¤šä¸ªé€‰æ‹©å™¨ï¼‰
            user_id = ""
            id_selectors = [
                "span:contains('æŠ–éŸ³å·')",
                "span:contains('æŠ–éŸ³å·ï¼š')",
                "div[class*='id']",
            ]
            for selector in id_selectors:
                # BeautifulSoup ä¸æ”¯æŒ :containsï¼Œéœ€è¦æ‰‹åŠ¨æŸ¥æ‰¾
                elems = item.find_all('span')
                for elem in elems:
                    text = elem.get_text(strip=True)
                    if 'æŠ–éŸ³å·' in text or 'douyin_id' in text:
                        # æå–å†’å·åé¢çš„å†…å®¹
                        if ':' in text or 'ï¼š' in text:
                            parts = text.split(':') if ':' in text else text.split('ï¼š')
                            if len(parts) > 1:
                                user_id = parts[1].strip()
                                break
                if user_id:
                    break

            # æå–ç²‰ä¸æ•°
            fans_count = 0
            stats_selectors = [
                "div.jjebLXt0 span",
                "span[class*='count']",
                "span[class*='fans']",
            ]
            for selector in stats_selectors:
                elems = item.select(selector)
                for elem in elems:
                    text = elem.get_text(strip=True)
                    if 'ç²‰ä¸' in text or 'fans' in text.lower():
                        # æå–æ•°å­—
                        import re
                        numbers = re.findall(r'[\d.]+ä¸‡?|[ä¸‡åƒ]', text)
                        if numbers:
                            fans_text = numbers[0]
                            # è½¬æ¢ä¸ºæ•°å­—
                            if 'ä¸‡' in fans_text:
                                fans_count = int(float(fans_text.replace('ä¸‡', '')) * 10000)
                            else:
                                fans_count = int(fans_text)
                            break

            # æå–è·èµæ•°
            likes_count = 0
            for elem in item.find_all('span'):
                text = elem.get_text(strip=True)
                if 'è·èµ' in text:
                    import re
                    numbers = re.findall(r'[\d.]+ä¸‡?', text)
                    if numbers:
                        likes_text = numbers[0]
                        if 'ä¸‡' in likes_text:
                            likes_count = int(float(likes_text.replace('ä¸‡', '')) * 10000)
                        else:
                            likes_count = int(likes_text)
                    break

            # æå–ç®€ä»‹
            description = ""
            desc_selectors = [
                "p.Kdb5Km3i span span span span span",
                "p[class*='desc']",
                "div[class*='desc']",
            ]
            for selector in desc_selectors:
                elem = item.select_one(selector)
                if elem:
                    description = elem.get_text(strip=True)
                    break

            # æå–å¤´åƒ URL
            avatar_url = ""
            avatar_elem = item.select_one('img')
            if avatar_elem and avatar_elem.get('src'):
                avatar_url = avatar_elem.get('src', '')

            return {
                'author': author,
                'user_id': user_id,
                'url': url,
                'fans_count': fans_count,
                'likes_count': likes_count,
                'description': description,
                'avatar_url': avatar_url,
            }

        except Exception as e:
            logger.warning(f"[douyin] Error extracting single user: {e}")
            return None

    def _parse_search_videos(self, soup: BeautifulSoup, limit: Optional[int] = None, user_id_filter: Optional[str] = None) -> List[Dict[str, Any]]:
        """ä»æœç´¢é¡µé¢è§£æè§†é¢‘æ•°æ®ï¼ˆä½¿ç”¨ BeautifulSoupï¼‰

        Args:
            soup: BeautifulSoup è§£æåçš„é¡µé¢å¯¹è±¡
            limit: é™åˆ¶è¿”å›æ•°é‡
            user_id_filter: å¯é€‰çš„ç”¨æˆ·IDè¿‡æ»¤

        Returns:
            List[Dict]: è§†é¢‘æ•°æ®åˆ—è¡¨
        """
        videos_data = []

        try:
            # å‚è€ƒ undoom çš„é€‰æ‹©å™¨ï¼ŒæŸ¥æ‰¾è§†é¢‘å¡ç‰‡
            possible_selectors = [
                "li.SwZLHMKk",  # undoom çš„é€‰æ‹©å™¨
                "div[class*='video-item']",
                "li[class*='video']",
                "div[data-e2e='search-video-card']",
            ]

            video_items = []
            for selector in possible_selectors:
                items = soup.select(selector)
                if items:
                    logger.info(f"[douyin] Found {len(items)} video items with selector: {selector}")
                    video_items = items
                    break

            if not video_items:
                logger.warning("[douyin] No video items found with any selector")
                logger.debug(f"[douyin] Page HTML preview: {str(soup)[:500]}")
                return []

            # è§£ææ¯ä¸ªè§†é¢‘å¡ç‰‡
            for item in video_items[:limit] if limit else video_items:
                try:
                    video_data = self._extract_single_video(item)
                    if video_data and video_data.get('title'):
                        # å¦‚æœæœ‰ç”¨æˆ·IDè¿‡æ»¤ï¼Œæ£€æŸ¥ä½œè€…
                        if user_id_filter and video_data.get('author') != user_id_filter:
                            continue
                        videos_data.append(video_data)
                except Exception as e:
                    logger.warning(f"[douyin] Failed to extract single video: {e}")
                    continue

            logger.info(f"[douyin] Successfully parsed {len(videos_data)} videos")

        except Exception as e:
            logger.error(f"[douyin] Error parsing search videos: {e}")

        return videos_data

    def _extract_single_video(self, item) -> Optional[Dict[str, Any]]:
        """ä»å•ä¸ªè§†é¢‘å¡ç‰‡å…ƒç´ æå–æ•°æ®

        Args:
            item: BeautifulSoup å…ƒç´ å¯¹è±¡

        Returns:
            Dict: è§†é¢‘æ•°æ®ï¼Œå¤±è´¥è¿”å› None
        """
        try:
            # æå–æ ‡é¢˜
            title = ""
            title_selectors = [
                "div.VDYK8Xd7",
                "span[class*='title']",
                "div[class*='title']",
                "a[class*='title']",
            ]
            for selector in title_selectors:
                elem = item.select_one(selector)
                if elem:
                    title = elem.get_text(strip=True)
                    break

            # æå–è§†é¢‘é“¾æ¥
            url = ""
            link_elem = item.select_one('a.hY8lWHgA') or item.select_one('a')
            if link_elem and link_elem.get('href'):
                href = link_elem.get('href', '')
                if href.startswith('//'):
                    url = 'https:' + href
                elif href.startswith('/'):
                    url = 'https://www.douyin.com' + href
                else:
                    url = href

            # æå–ä½œè€…
            author = ""
            author_selectors = [
                "span.MZNczJmS",
                "span[class*='author']",
                "a[class*='author']",
            ]
            for selector in author_selectors:
                elem = item.select_one(selector)
                if elem:
                    author = elem.get_text(strip=True)
                    break

            # æå–ç‚¹èµæ•°
            liked_count = "0"
            likes_elem = item.select_one('span.cIiU4Muu')
            if likes_elem:
                liked_count = likes_elem.get_text(strip=True)

            # æå–å‘å¸ƒæ—¶é—´
            publish_time = ""
            time_elem = item.select_one('span.faDtinfi')
            if time_elem:
                publish_time = time_elem.get_text(strip=True)

            return {
                'title': title,
                'url': url,
                'author': author,
                'liked_count': liked_count,
                'publish_time': publish_time,
            }

        except Exception as e:
            logger.warning(f"[douyin] Error extracting single video: {e}")
            return None

    def _parse_video_detail(self, soup: BeautifulSoup, js_data: Optional[Dict] = None) -> Optional[Dict[str, Any]]:
        """ä»è§†é¢‘è¯¦æƒ…é¡µé¢è§£æè§†é¢‘æ•°æ®ï¼ˆæ”¹è¿›ç‰ˆï¼šæ”¯æŒä» script æ ‡ç­¾ä¸­æå–æ•°æ®ï¼‰

        Args:
            soup: BeautifulSoup è§£æåçš„é¡µé¢å¯¹è±¡
            js_data: å¯é€‰çš„ä»é¡µé¢JavaScriptè·å–çš„æ•°æ®

        Returns:
            Dict: è§†é¢‘è¯¦æƒ…æ•°æ®ï¼Œå¤±è´¥è¿”å› None
        """
        try:
            # 1. ä¼˜å…ˆä½¿ç”¨ç›´æ¥ä»JavaScriptè·å–çš„æ•°æ®
            if js_data and js_data.get('type') != 'none':
                logger.info(f"[douyin] Using JavaScript data: {js_data.get('type')}")
                video_data = self._parse_js_data(js_data)
                if video_data:
                    return video_data

            # 2. å°è¯•ä» script æ ‡ç­¾ä¸­æå–æ•°æ®ï¼ˆæŠ–éŸ³çš„æ•°æ®é€šå¸¸å­˜å‚¨åœ¨ window.__RENDER_DATA__ ä¸­ï¼‰
            video_data = self._extract_from_script_tags(soup)
            if video_data:
                return video_data

            # 3. å¦‚æœ script æ ‡ç­¾æå–å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ CSS é€‰æ‹©å™¨ä» HTML ä¸­æå–
            return self._extract_from_html(soup)

        except Exception as e:
            logger.error(f"[douyin] Error parsing video detail: {e}")
            import traceback
            logger.error(f"[douyin] Traceback: {traceback.format_exc()}")
            return None

    def _extract_from_script_tags(self, soup: BeautifulSoup) -> Optional[Dict[str, Any]]:
        """ä» script æ ‡ç­¾ä¸­æå–è§†é¢‘æ•°æ®ï¼ˆæŠ–éŸ³çš„æ•°æ®é€šå¸¸åœ¨ window.__RENDER_DATA__ ä¸­ï¼‰"""
        try:
            import re
            import json

            # æŸ¥æ‰¾æ‰€æœ‰ script æ ‡ç­¾
            scripts = soup.find_all('script')

            for script in scripts:
                if not script.string:
                    continue

                script_content = script.string

                # å°è¯•æå– window.__RENDER_DATA__
                render_data_match = re.search(r'window\.__RENDER_DATA__\s*=\s*({.*?});', script_content, re.DOTALL)
                if render_data_match:
                    try:
                        render_data = json.loads(render_data_match.group(1))
                        logger.info("[douyin] Found data in window.__RENDER_DATA__")

                        # è§£ææ•°æ®ç»“æ„
                        data = self._parse_render_data(render_data)
                        if data:
                            return data
                    except json.JSONDecodeError as e:
                        logger.warning(f"[douyin] Failed to parse __RENDER_DATA__ JSON: {e}")
                        continue

                # å°è¯•æå–å…¶ä»–å¯èƒ½çš„æ•°æ®æ ¼å¼
                # ä¾‹å¦‚ï¼šwindow.SSR_RENDER_DATA, window.INITIAL_STATE ç­‰
                ssr_match = re.search(r'window\.SSR_RENDER_DATA\s*=\s*({.*?});', script_content, re.DOTALL)
                if ssr_match:
                    try:
                        ssr_data = json.loads(ssr_match.group(1))
                        logger.info("[douyin] Found data in window.SSR_RENDER_DATA")
                        data = self._parse_render_data(ssr_data)
                        if data:
                            return data
                    except json.JSONDecodeError:
                        continue

            logger.warning("[douyin] No data found in script tags")
            return None

        except Exception as e:
            logger.error(f"[douyin] Error extracting from script tags: {e}")
            return None

    def _parse_js_data(self, js_data: dict) -> Optional[Dict[str, Any]]:
        """è§£æä»é¡µé¢JavaScriptç›´æ¥è·å–çš„æ•°æ®"""
        try:
            if not js_data or js_data.get('type') == 'none':
                return None

            data = js_data.get('data')
            if not data:
                return None

            logger.info(f"[douyin] Parsing JavaScript data, type: {js_data.get('type')}")

            # é€’å½’æŸ¥æ‰¾è§†é¢‘æ•°æ®
            video_info = self._find_video_in_data(data)

            logger.info(f"video_info {video_info}")

            if not video_info:
                logger.warning("[douyin] Video info not found in JavaScript data")
                # æ‰“å°è°ƒè¯•ä¿¡æ¯
                logger.debug(f"[douyin] JS data structure: {str(type(data))} - {str(data)[:200] if isinstance(data, (dict, str)) else type(data)}")
                return None

            # æå–æ•°æ®
            video_id = video_info.get('aweme_id') or video_info.get('video_id') or ''

            # æ ‡é¢˜å’Œæè¿°
            desc = video_info.get('desc') or ''

            # ä½œè€…ä¿¡æ¯
            author_info = video_info.get('author', {})
            author = author_info.get('nickname') or author_info.get('unique_id') or author_info.get('signature', '')

            # ç»Ÿè®¡æ•°æ®
            statistics = video_info.get('statistics', {})
            liked_count = str(statistics.get('digg_count') or statistics.get('diggCount') or 0)
            comment_count = str(statistics.get('comment_count') or statistics.get('commentCount') or 0)
            share_count = str(statistics.get('share_count') or statistics.get('shareCount') or 0)
            collect_count = str(statistics.get('collect_count') or statistics.get('collectCount') or 0)

            logger.info(f"[douyin] Successfully extracted from JavaScript: video_id={video_id}, title={desc[:30] if desc else 'N/A'}")

            return {
                'video_id': video_id,
                'title': desc,
                'desc': desc,
                'author': author,
                'liked_count': liked_count,
                'comment_count': comment_count,
                'share_count': share_count,
                'collect_count': collect_count,
            }

        except Exception as e:
            logger.error(f"[douyin] Error parsing JavaScript data: {e}")
            import traceback
            logger.error(f"[douyin] Traceback: {traceback.format_exc()}")
            return None

    def _find_video_in_data(self, obj, depth=0):
        """é€’å½’æŸ¥æ‰¾è§†é¢‘æ•°æ®å¯¹è±¡"""
        if depth > 15:  # é˜²æ­¢æ— é™é€’å½’
            return None

        if isinstance(obj, dict):
            # æŸ¥æ‰¾åŒ…å«è§†é¢‘ä¿¡æ¯çš„å­—å…¸
            if 'aweme_id' in obj or 'video_id' in obj or ('desc' in obj and 'author' in obj):
                return obj

            # é€’å½’æŸ¥æ‰¾
            for key, value in obj.items():
                result = self._find_video_in_data(value, depth + 1)
                if result:
                    return result

        elif isinstance(obj, list):
            for item in obj:
                result = self._find_video_in_data(item, depth + 1)
                if result:
                    return result

        return None

    def _parse_render_data(self, render_data: dict) -> Optional[Dict[str, Any]]:
        """è§£æä» window.__RENDER_DATA__ ä¸­æå–çš„æ•°æ®"""
        try:
            # æŠ–éŸ³çš„æ•°æ®ç»“æ„å¯èƒ½æœ‰å¤šå±‚åµŒå¥—ï¼Œéœ€è¦é€’å½’æŸ¥æ‰¾
            video_info = self._find_video_in_data(render_data)

            if not video_info:
                logger.warning("[douyin] Video info not found in render data")
                return None

            # æå–æ•°æ®
            video_id = video_info.get('aweme_id') or video_info.get('video_id') or ''

            # æ ‡é¢˜å’Œæè¿°
            desc = video_info.get('desc') or ''

            # ä½œè€…ä¿¡æ¯
            author_info = video_info.get('author', {})
            author = author_info.get('nickname') or author_info.get('unique_id') or author_info.get('signature', '')

            # ç»Ÿè®¡æ•°æ®
            statistics = video_info.get('statistics', {})
            liked_count = str(statistics.get('digg_count') or statistics.get('diggCount') or 0)
            comment_count = str(statistics.get('comment_count') or statistics.get('commentCount') or 0)
            share_count = str(statistics.get('share_count') or statistics.get('shareCount') or 0)
            collect_count = str(statistics.get('collect_count') or statistics.get('collectCount') or 0)

            logger.info(f"[douyin] Successfully extracted from script: video_id={video_id}, title={desc[:30] if desc else 'N/A'}")

            return {
                'video_id': video_id,
                'title': desc,  # æŠ–éŸ³çš„ desc å®é™…ä¸Šå°±æ˜¯è§†é¢‘æ ‡é¢˜/æè¿°
                'desc': desc,
                'author': author,
                'liked_count': liked_count,
                'comment_count': comment_count,
                'share_count': share_count,
                'collect_count': collect_count,
            }

        except Exception as e:
            logger.error(f"[douyin] Error parsing render data: {e}")
            return None

    def _extract_from_html(self, soup: BeautifulSoup) -> Optional[Dict[str, Any]]:
        """ä» HTML å…ƒç´ ä¸­æå–è§†é¢‘æ•°æ®ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
        try:
            # æå–è§†é¢‘IDï¼ˆå°è¯•ä» URL æˆ–é¡µé¢ä¸­è·å–ï¼‰
            video_id = ""
            scripts = soup.find_all('script')
            for script in scripts:
                if script.string and 'video_id' in script.string:
                    import re
                    match = re.search(r'"video_id"\s*:\s*"([^"]+)"', script.string)
                    if match:
                        video_id = match.group(1)
                        break

            # æå–è§†é¢‘æ ‡é¢˜/æè¿° - æ”¹è¿›çš„é€‰æ‹©å™¨
            title = ""
            desc = ""

            # å°è¯•æ›´å¤šå¯èƒ½çš„é€‰æ‹©å™¨
            title_selectors = [
                "h1[class*='title']",
                "div[class*='title']",
                "span[class*='desc']",
                "div[class*='desc']",
                "[data-e2e='video-desc']",
                ".video-desc",
                ".desc",
            ]
            for selector in title_selectors:
                elem = soup.select_one(selector)
                if elem:
                    text = elem.get_text(strip=True)
                    if text and len(text) > 0:
                        if not title:
                            title = text
                        elif not desc:
                            desc = text
                        break

            # å¦‚æœæ ‡é¢˜å’Œæè¿°éƒ½ç›¸åŒï¼Œåªä¿ç•™æ ‡é¢˜
            if title == desc:
                desc = ""

            # æå–ä½œè€…ä¿¡æ¯ - æ”¹è¿›çš„é€‰æ‹©å™¨
            author = ""
            author_selectors = [
                "a[class*='author']",
                "span[class*='author']",
                "div[class*='author']",
                "a[href*='/user/']",
                "[data-e2e='video-author']",
                ".author-name",
                ".user-name",
            ]
            for selector in author_selectors:
                elem = soup.select_one(selector)
                if elem:
                    author = elem.get_text(strip=True)
                    if author and len(author) > 0:
                        break

            # æå–ç‚¹èµæ•° - æ”¹è¿›çš„é€‰æ‹©å™¨
            liked_count = "0"
            like_selectors = [
                "span[class*='like']",
                "span[data-e2e='video-like-count']",
                "div[class*='like'] span",
                "[data-e2e='browse-like-count']",
            ]
            for selector in like_selectors:
                elem = soup.select_one(selector)
                if elem:
                    text = elem.get_text(strip=True)
                    if text and any(c.isdigit() for c in text):
                        liked_count = text
                        break

            # æå–è¯„è®ºæ•° - æ”¹è¿›çš„é€‰æ‹©å™¨
            comment_count = "0"
            comment_selectors = [
                "span[class*='comment']",
                "span[data-e2e='video-comment-count']",
                "div[class*='comment'] span",
                "[data-e2e='browse-comment-count']",
            ]
            for selector in comment_selectors:
                elem = soup.select_one(selector)
                if elem:
                    text = elem.get_text(strip=True)
                    if text and any(c.isdigit() for c in text):
                        comment_count = text
                        break

            # æå–åˆ†äº«æ•° - æ”¹è¿›çš„é€‰æ‹©å™¨
            share_count = "0"
            share_selectors = [
                "span[class*='share']",
                "span[data-e2e='video-share-count']",
                "div[class*='share'] span",
            ]
            for selector in share_selectors:
                elem = soup.select_one(selector)
                if elem:
                    text = elem.get_text(strip=True)
                    if text and any(c.isdigit() for c in text):
                        share_count = text
                        break

            # æå–æ”¶è—æ•° - æ”¹è¿›çš„é€‰æ‹©å™¨
            collect_count = "0"
            collect_selectors = [
                "span[class*='collect']",
                "span[data-e2e='video-collect-count']",
                "div[class*='collect'] span",
            ]
            for selector in collect_selectors:
                elem = soup.select_one(selector)
                if elem:
                    text = elem.get_text(strip=True)
                    if text and any(c.isdigit() for c in text):
                        collect_count = text
                        break

            # è‡³å°‘è¦æœ‰æ ‡é¢˜æˆ–ä½œè€…ä¹‹ä¸€
            if not title and not author:
                logger.warning("[douyin] Video detail missing both title and author")
                return None

            logger.info(f"[douyin] Successfully extracted from HTML: title={title[:30] if title else 'N/A'}")

            return {
                'video_id': video_id,
                'title': title,
                'desc': desc,
                'author': author,
                'liked_count': liked_count,
                'comment_count': comment_count,
                'share_count': share_count,
                'collect_count': collect_count,
            }

        except Exception as e:
            logger.error(f"[douyin] Error extracting from HTML: {e}")
            import traceback
            logger.error(f"[douyin] Traceback: {traceback.format_exc()}")
            return None

